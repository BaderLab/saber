{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "!PYTHONPATH=../kari"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sequence_processing_model'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-a5b4c3faa04d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msequence_processing_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequenceProcessingModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mutils_parameter_parsing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sequence_processing_model'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sequence_processing_model import SequenceProcessingModel\n",
    "from dataset import Dataset\n",
    "from utils_parameter_parsing import *\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the parameters from the config file and create a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset... \n",
      "\n",
      "Done (0.81 seconds)\n",
      "Loading embeddings... found 1891782 word vectors of dimension 200.\n",
      "Done (113.74 seconds)\n",
      "WARNING:tensorflow:From /Users/johngiorgi/miniconda3/envs/learning_keras/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1247: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /Users/johngiorgi/miniconda3/envs/learning_keras/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1230: calling reduce_min (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /Users/johngiorgi/miniconda3/envs/learning_keras/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1364: calling reduce_any (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /Users/johngiorgi/miniconda3/envs/learning_keras/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1489: calling reduce_logsumexp (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /Users/johngiorgi/miniconda3/envs/learning_keras/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1349: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 50, 200)           2164000   \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 50, 200)           240800    \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 50, 100)           20100     \n",
      "_________________________________________________________________\n",
      "crf_1 (CRF)                  (None, 50, 5)             540       \n",
      "=================================================================\n",
      "Total params: 2,425,440\n",
      "Trainable params: 261,440\n",
      "Non-trainable params: 2,164,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "config = config_parser('./config.ini') # parse config.ini\n",
    "parameters = process_parameters(config, {}) # process_parameters expects a dictionary\n",
    "\n",
    "sequence_processing_model = SequenceProcessingModel(**parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5712 samples, validate on 635 samples\n",
      "Epoch 1/50\n",
      "5712/5712 [==============================] - 392s 69ms/step - loss: 3.2145 - acc: 0.9120 - val_loss: 2.6345 - val_acc: 0.9243\n",
      "Epoch 2/50\n",
      "5712/5712 [==============================] - 388s 68ms/step - loss: 3.0998 - acc: 0.9277 - val_loss: 2.5826 - val_acc: 0.9399\n",
      "Epoch 3/50\n",
      "5712/5712 [==============================] - 388s 68ms/step - loss: 3.0591 - acc: 0.9401 - val_loss: 2.5517 - val_acc: 0.9542\n",
      "Epoch 4/50\n",
      "5712/5712 [==============================] - 392s 69ms/step - loss: 3.0344 - acc: 0.9485 - val_loss: 2.5344 - val_acc: 0.9576\n",
      "Epoch 5/50\n",
      "5712/5712 [==============================] - 388s 68ms/step - loss: 3.0173 - acc: 0.9526 - val_loss: 2.5216 - val_acc: 0.9599\n",
      "Epoch 6/50\n",
      "5712/5712 [==============================] - 379s 66ms/step - loss: 3.0047 - acc: 0.9567 - val_loss: 2.5129 - val_acc: 0.9610\n",
      "Epoch 7/50\n",
      "5712/5712 [==============================] - 367s 64ms/step - loss: 2.9952 - acc: 0.9602 - val_loss: 2.5103 - val_acc: 0.9603\n",
      "Epoch 8/50\n",
      "5712/5712 [==============================] - 367s 64ms/step - loss: 2.9877 - acc: 0.9617 - val_loss: 2.5023 - val_acc: 0.9619\n",
      "Epoch 9/50\n",
      "5712/5712 [==============================] - 367s 64ms/step - loss: 2.9815 - acc: 0.9642 - val_loss: 2.4978 - val_acc: 0.9626\n",
      "Epoch 10/50\n",
      "5712/5712 [==============================] - 367s 64ms/step - loss: 2.9764 - acc: 0.9655 - val_loss: 2.4926 - val_acc: 0.9648\n",
      "Epoch 11/50\n",
      "5712/5712 [==============================] - 367s 64ms/step - loss: 2.9722 - acc: 0.9674 - val_loss: 2.4905 - val_acc: 0.9637\n",
      "Epoch 12/50\n",
      "5712/5712 [==============================] - 388s 68ms/step - loss: 2.9685 - acc: 0.9680 - val_loss: 2.4862 - val_acc: 0.9658\n",
      "Epoch 13/50\n",
      "5712/5712 [==============================] - 390s 68ms/step - loss: 2.9653 - acc: 0.9692 - val_loss: 2.4839 - val_acc: 0.9662\n",
      "Epoch 14/50\n",
      "5712/5712 [==============================] - 390s 68ms/step - loss: 2.9626 - acc: 0.9704 - val_loss: 2.4843 - val_acc: 0.9654\n",
      "Epoch 15/50\n",
      "5712/5712 [==============================] - 393s 69ms/step - loss: 2.9602 - acc: 0.9703 - val_loss: 2.4816 - val_acc: 0.9662\n",
      "Epoch 16/50\n",
      "5712/5712 [==============================] - 393s 69ms/step - loss: 2.9579 - acc: 0.9719 - val_loss: 2.4796 - val_acc: 0.9679\n",
      "Epoch 17/50\n",
      "5712/5712 [==============================] - 390s 68ms/step - loss: 2.9561 - acc: 0.9723 - val_loss: 2.4806 - val_acc: 0.9656\n",
      "Epoch 18/50\n",
      "5712/5712 [==============================] - 390s 68ms/step - loss: 2.9543 - acc: 0.9730 - val_loss: 2.4778 - val_acc: 0.9676\n",
      "Epoch 19/50\n",
      "5712/5712 [==============================] - 394s 69ms/step - loss: 2.9527 - acc: 0.9731 - val_loss: 2.4771 - val_acc: 0.9692\n",
      "Epoch 20/50\n",
      "5712/5712 [==============================] - 390s 68ms/step - loss: 2.9514 - acc: 0.9742 - val_loss: 2.4799 - val_acc: 0.9655\n",
      "Epoch 21/50\n",
      "5712/5712 [==============================] - 390s 68ms/step - loss: 2.9500 - acc: 0.9743 - val_loss: 2.4763 - val_acc: 0.9679\n",
      "Epoch 22/50\n",
      "5712/5712 [==============================] - 391s 68ms/step - loss: 2.9487 - acc: 0.9750 - val_loss: 2.4789 - val_acc: 0.9655\n",
      "Epoch 23/50\n",
      "5712/5712 [==============================] - 390s 68ms/step - loss: 2.9476 - acc: 0.9756 - val_loss: 2.4747 - val_acc: 0.9680\n",
      "Epoch 24/50\n",
      "5712/5712 [==============================] - 394s 69ms/step - loss: 2.9465 - acc: 0.9759 - val_loss: 2.4732 - val_acc: 0.9705\n",
      "Epoch 25/50\n",
      "5712/5712 [==============================] - 393s 69ms/step - loss: 2.9456 - acc: 0.9764 - val_loss: 2.4751 - val_acc: 0.9678\n",
      "Epoch 26/50\n",
      "5712/5712 [==============================] - 391s 68ms/step - loss: 2.9445 - acc: 0.9769 - val_loss: 2.4726 - val_acc: 0.9692\n",
      "Epoch 27/50\n",
      "5712/5712 [==============================] - 391s 68ms/step - loss: 2.9438 - acc: 0.9768 - val_loss: 2.4725 - val_acc: 0.9686\n",
      "Epoch 28/50\n",
      "5712/5712 [==============================] - 391s 68ms/step - loss: 2.9429 - acc: 0.9776 - val_loss: 2.4739 - val_acc: 0.9680\n",
      "Epoch 29/50\n",
      "5712/5712 [==============================] - 393s 69ms/step - loss: 2.9421 - acc: 0.9778 - val_loss: 2.4726 - val_acc: 0.9689\n",
      "Epoch 30/50\n",
      "5712/5712 [==============================] - 390s 68ms/step - loss: 2.9413 - acc: 0.9784 - val_loss: 2.4720 - val_acc: 0.9698\n",
      "Epoch 31/50\n",
      "5712/5712 [==============================] - 391s 68ms/step - loss: 2.9407 - acc: 0.9785 - val_loss: 2.4722 - val_acc: 0.9692\n",
      "Epoch 32/50\n",
      "5712/5712 [==============================] - 390s 68ms/step - loss: 2.9400 - acc: 0.9789 - val_loss: 2.4713 - val_acc: 0.9700\n",
      "Epoch 33/50\n",
      "5712/5712 [==============================] - 393s 69ms/step - loss: 2.9393 - acc: 0.9792 - val_loss: 2.4714 - val_acc: 0.9692\n",
      "Epoch 34/50\n",
      "5712/5712 [==============================] - 391s 68ms/step - loss: 2.9386 - acc: 0.9795 - val_loss: 2.4707 - val_acc: 0.9706\n",
      "Epoch 35/50\n",
      "5712/5712 [==============================] - 391s 68ms/step - loss: 2.9380 - acc: 0.9800 - val_loss: 2.4699 - val_acc: 0.9713\n",
      "Epoch 36/50\n",
      "5712/5712 [==============================] - 390s 68ms/step - loss: 2.9375 - acc: 0.9806 - val_loss: 2.4714 - val_acc: 0.9702\n",
      "Epoch 37/50\n",
      "5712/5712 [==============================] - 393s 69ms/step - loss: 2.9369 - acc: 0.9800 - val_loss: 2.4714 - val_acc: 0.9698\n",
      "Epoch 38/50\n",
      "5712/5712 [==============================] - 391s 68ms/step - loss: 2.9363 - acc: 0.9803 - val_loss: 2.4695 - val_acc: 0.9713\n",
      "Epoch 39/50\n",
      "5712/5712 [==============================] - 391s 69ms/step - loss: 2.9359 - acc: 0.9804 - val_loss: 2.4717 - val_acc: 0.9699\n",
      "Epoch 40/50\n",
      "5712/5712 [==============================] - 391s 68ms/step - loss: 2.9354 - acc: 0.9808 - val_loss: 2.4690 - val_acc: 0.9716\n",
      "Epoch 41/50\n",
      "5712/5712 [==============================] - 391s 68ms/step - loss: 2.9349 - acc: 0.9812 - val_loss: 2.4719 - val_acc: 0.9686\n",
      "Epoch 42/50\n",
      "5712/5712 [==============================] - 391s 68ms/step - loss: 2.9344 - acc: 0.9818 - val_loss: 2.4683 - val_acc: 0.9731\n",
      "Epoch 43/50\n",
      "5712/5712 [==============================] - 394s 69ms/step - loss: 2.9339 - acc: 0.9819 - val_loss: 2.4689 - val_acc: 0.9721\n",
      "Epoch 44/50\n",
      "5712/5712 [==============================] - 391s 68ms/step - loss: 2.9334 - acc: 0.9817 - val_loss: 2.4701 - val_acc: 0.9708\n",
      "Epoch 45/50\n",
      "5712/5712 [==============================] - 394s 69ms/step - loss: 2.9331 - acc: 0.9822 - val_loss: 2.4692 - val_acc: 0.9717\n",
      "Epoch 46/50\n",
      "5712/5712 [==============================] - 391s 69ms/step - loss: 2.9326 - acc: 0.9825 - val_loss: 2.4676 - val_acc: 0.9717\n",
      "Epoch 47/50\n",
      "5712/5712 [==============================] - 391s 69ms/step - loss: 2.9322 - acc: 0.9827 - val_loss: 2.4676 - val_acc: 0.9729\n",
      "Epoch 48/50\n",
      "5712/5712 [==============================] - 391s 68ms/step - loss: 2.9318 - acc: 0.9828 - val_loss: 2.4705 - val_acc: 0.9706\n",
      "Epoch 49/50\n",
      "5712/5712 [==============================] - 394s 69ms/step - loss: 2.9314 - acc: 0.9832 - val_loss: 2.4679 - val_acc: 0.9733\n",
      "Epoch 50/50\n",
      "5712/5712 [==============================] - 392s 69ms/step - loss: 2.9310 - acc: 0.9831 - val_loss: 2.4715 - val_acc: 0.9707\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc</th>\n",
       "      <th>loss</th>\n",
       "      <th>val_acc</th>\n",
       "      <th>val_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.912048</td>\n",
       "      <td>3.214479</td>\n",
       "      <td>0.924322</td>\n",
       "      <td>2.634516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.927744</td>\n",
       "      <td>3.099762</td>\n",
       "      <td>0.939908</td>\n",
       "      <td>2.582646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.940103</td>\n",
       "      <td>3.059051</td>\n",
       "      <td>0.954207</td>\n",
       "      <td>2.551660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.948495</td>\n",
       "      <td>3.034433</td>\n",
       "      <td>0.957628</td>\n",
       "      <td>2.534413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.952603</td>\n",
       "      <td>3.017336</td>\n",
       "      <td>0.959899</td>\n",
       "      <td>2.521640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.956655</td>\n",
       "      <td>3.004738</td>\n",
       "      <td>0.961038</td>\n",
       "      <td>2.512852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.960181</td>\n",
       "      <td>2.995198</td>\n",
       "      <td>0.960332</td>\n",
       "      <td>2.510276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.961652</td>\n",
       "      <td>2.987671</td>\n",
       "      <td>0.961883</td>\n",
       "      <td>2.502305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.964205</td>\n",
       "      <td>2.981507</td>\n",
       "      <td>0.962621</td>\n",
       "      <td>2.497823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.965516</td>\n",
       "      <td>2.976375</td>\n",
       "      <td>0.964829</td>\n",
       "      <td>2.492595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.967400</td>\n",
       "      <td>2.972168</td>\n",
       "      <td>0.963729</td>\n",
       "      <td>2.490510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.967966</td>\n",
       "      <td>2.968507</td>\n",
       "      <td>0.965764</td>\n",
       "      <td>2.486152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.969212</td>\n",
       "      <td>2.965343</td>\n",
       "      <td>0.966201</td>\n",
       "      <td>2.483942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.970441</td>\n",
       "      <td>2.962634</td>\n",
       "      <td>0.965449</td>\n",
       "      <td>2.484280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.970267</td>\n",
       "      <td>2.960182</td>\n",
       "      <td>0.966194</td>\n",
       "      <td>2.481566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.971925</td>\n",
       "      <td>2.957908</td>\n",
       "      <td>0.967916</td>\n",
       "      <td>2.479581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.972327</td>\n",
       "      <td>2.956076</td>\n",
       "      <td>0.965602</td>\n",
       "      <td>2.480625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.972974</td>\n",
       "      <td>2.954257</td>\n",
       "      <td>0.967593</td>\n",
       "      <td>2.477841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.973128</td>\n",
       "      <td>2.952738</td>\n",
       "      <td>0.969157</td>\n",
       "      <td>2.477135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.974244</td>\n",
       "      <td>2.951419</td>\n",
       "      <td>0.965489</td>\n",
       "      <td>2.479900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.974346</td>\n",
       "      <td>2.949990</td>\n",
       "      <td>0.967910</td>\n",
       "      <td>2.476283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.975008</td>\n",
       "      <td>2.948714</td>\n",
       "      <td>0.965462</td>\n",
       "      <td>2.478880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.975590</td>\n",
       "      <td>2.947601</td>\n",
       "      <td>0.967965</td>\n",
       "      <td>2.474730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.975877</td>\n",
       "      <td>2.946533</td>\n",
       "      <td>0.970540</td>\n",
       "      <td>2.473161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.976399</td>\n",
       "      <td>2.945577</td>\n",
       "      <td>0.967764</td>\n",
       "      <td>2.475067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.976855</td>\n",
       "      <td>2.944532</td>\n",
       "      <td>0.969229</td>\n",
       "      <td>2.472644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.976781</td>\n",
       "      <td>2.943822</td>\n",
       "      <td>0.968642</td>\n",
       "      <td>2.472465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.977613</td>\n",
       "      <td>2.942886</td>\n",
       "      <td>0.968005</td>\n",
       "      <td>2.473861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.977813</td>\n",
       "      <td>2.942110</td>\n",
       "      <td>0.968932</td>\n",
       "      <td>2.472632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.978390</td>\n",
       "      <td>2.941347</td>\n",
       "      <td>0.969839</td>\n",
       "      <td>2.471958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.978481</td>\n",
       "      <td>2.940664</td>\n",
       "      <td>0.969178</td>\n",
       "      <td>2.472179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.978946</td>\n",
       "      <td>2.940016</td>\n",
       "      <td>0.970048</td>\n",
       "      <td>2.471316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.979207</td>\n",
       "      <td>2.939344</td>\n",
       "      <td>0.969193</td>\n",
       "      <td>2.471429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.979515</td>\n",
       "      <td>2.938641</td>\n",
       "      <td>0.970616</td>\n",
       "      <td>2.470722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.979957</td>\n",
       "      <td>2.938005</td>\n",
       "      <td>0.971332</td>\n",
       "      <td>2.469928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.980633</td>\n",
       "      <td>2.937457</td>\n",
       "      <td>0.970212</td>\n",
       "      <td>2.471383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.980045</td>\n",
       "      <td>2.936941</td>\n",
       "      <td>0.969813</td>\n",
       "      <td>2.471421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.980285</td>\n",
       "      <td>2.936263</td>\n",
       "      <td>0.971317</td>\n",
       "      <td>2.469517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.980401</td>\n",
       "      <td>2.935924</td>\n",
       "      <td>0.969874</td>\n",
       "      <td>2.471745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.980781</td>\n",
       "      <td>2.935395</td>\n",
       "      <td>0.971614</td>\n",
       "      <td>2.469037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.981156</td>\n",
       "      <td>2.934895</td>\n",
       "      <td>0.968592</td>\n",
       "      <td>2.471890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.981822</td>\n",
       "      <td>2.934388</td>\n",
       "      <td>0.973086</td>\n",
       "      <td>2.468316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.981946</td>\n",
       "      <td>2.933877</td>\n",
       "      <td>0.972100</td>\n",
       "      <td>2.468890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.981713</td>\n",
       "      <td>2.933438</td>\n",
       "      <td>0.970811</td>\n",
       "      <td>2.470109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.982218</td>\n",
       "      <td>2.933096</td>\n",
       "      <td>0.971668</td>\n",
       "      <td>2.469203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.982469</td>\n",
       "      <td>2.932608</td>\n",
       "      <td>0.971706</td>\n",
       "      <td>2.467624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.982747</td>\n",
       "      <td>2.932167</td>\n",
       "      <td>0.972888</td>\n",
       "      <td>2.467592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.982845</td>\n",
       "      <td>2.931787</td>\n",
       "      <td>0.970629</td>\n",
       "      <td>2.470524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.983187</td>\n",
       "      <td>2.931386</td>\n",
       "      <td>0.973294</td>\n",
       "      <td>2.467949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.983061</td>\n",
       "      <td>2.931001</td>\n",
       "      <td>0.970675</td>\n",
       "      <td>2.471469</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         acc      loss   val_acc  val_loss\n",
       "0   0.912048  3.214479  0.924322  2.634516\n",
       "1   0.927744  3.099762  0.939908  2.582646\n",
       "2   0.940103  3.059051  0.954207  2.551660\n",
       "3   0.948495  3.034433  0.957628  2.534413\n",
       "4   0.952603  3.017336  0.959899  2.521640\n",
       "5   0.956655  3.004738  0.961038  2.512852\n",
       "6   0.960181  2.995198  0.960332  2.510276\n",
       "7   0.961652  2.987671  0.961883  2.502305\n",
       "8   0.964205  2.981507  0.962621  2.497823\n",
       "9   0.965516  2.976375  0.964829  2.492595\n",
       "10  0.967400  2.972168  0.963729  2.490510\n",
       "11  0.967966  2.968507  0.965764  2.486152\n",
       "12  0.969212  2.965343  0.966201  2.483942\n",
       "13  0.970441  2.962634  0.965449  2.484280\n",
       "14  0.970267  2.960182  0.966194  2.481566\n",
       "15  0.971925  2.957908  0.967916  2.479581\n",
       "16  0.972327  2.956076  0.965602  2.480625\n",
       "17  0.972974  2.954257  0.967593  2.477841\n",
       "18  0.973128  2.952738  0.969157  2.477135\n",
       "19  0.974244  2.951419  0.965489  2.479900\n",
       "20  0.974346  2.949990  0.967910  2.476283\n",
       "21  0.975008  2.948714  0.965462  2.478880\n",
       "22  0.975590  2.947601  0.967965  2.474730\n",
       "23  0.975877  2.946533  0.970540  2.473161\n",
       "24  0.976399  2.945577  0.967764  2.475067\n",
       "25  0.976855  2.944532  0.969229  2.472644\n",
       "26  0.976781  2.943822  0.968642  2.472465\n",
       "27  0.977613  2.942886  0.968005  2.473861\n",
       "28  0.977813  2.942110  0.968932  2.472632\n",
       "29  0.978390  2.941347  0.969839  2.471958\n",
       "30  0.978481  2.940664  0.969178  2.472179\n",
       "31  0.978946  2.940016  0.970048  2.471316\n",
       "32  0.979207  2.939344  0.969193  2.471429\n",
       "33  0.979515  2.938641  0.970616  2.470722\n",
       "34  0.979957  2.938005  0.971332  2.469928\n",
       "35  0.980633  2.937457  0.970212  2.471383\n",
       "36  0.980045  2.936941  0.969813  2.471421\n",
       "37  0.980285  2.936263  0.971317  2.469517\n",
       "38  0.980401  2.935924  0.969874  2.471745\n",
       "39  0.980781  2.935395  0.971614  2.469037\n",
       "40  0.981156  2.934895  0.968592  2.471890\n",
       "41  0.981822  2.934388  0.973086  2.468316\n",
       "42  0.981946  2.933877  0.972100  2.468890\n",
       "43  0.981713  2.933438  0.970811  2.470109\n",
       "44  0.982218  2.933096  0.971668  2.469203\n",
       "45  0.982469  2.932608  0.971706  2.467624\n",
       "46  0.982747  2.932167  0.972888  2.467592\n",
       "47  0.982845  2.931787  0.970629  2.470524\n",
       "48  0.983187  2.931386  0.973294  2.467949\n",
       "49  0.983061  2.931001  0.970675  2.471469"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence_processing_model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9853404255319149\n"
     ]
    }
   ],
   "source": [
    "pred_idx, gold_idx, labels_ = sequence_processing_model.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9853404255319149"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.count_nonzero((pred_idx == gold_idx)) / len(pred_idx)\n",
    "# gold_idx[:18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'B-Disease': 2, 'E-Disease': 0, 'I-Disease': 1, 'O': 4, 'S-Disease': 3}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence_processing_model.ds.tag_type_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_tag = sequence_processing_model.ds.tag_type_to_index['O']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False, False, False, False, False,  True,  True,  True,\n",
       "       False, False, False,  True,  True,  True,  True,  True, False])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gold_labels = gold_idx != neg_tag\n",
    "gold_labels[:18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False, False, False, False, False,  True,  True,  True,\n",
       "       False, False, False,  True,  True,  True,  True,  True, False])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_labels = pred_idx != neg_tag\n",
    "pred_labels[:18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "TP = 0\n",
    "FP = 0\n",
    "FN = 0\n",
    "\n",
    "for pred, gold in zip(pred_labels, gold_labels):\n",
    "    # FN\n",
    "    if gold and not pred:\n",
    "        FN += 1\n",
    "    # FP\n",
    "    if not gold and pred:\n",
    "        FP += 1\n",
    "    # TP\n",
    "    if gold and pred:\n",
    "        TP += 1\n",
    "        \n",
    "precision = TP / (TP + FP)\n",
    "recall = TP / (TP + FN)\n",
    "f1 = 2 * precision * recall / (precision + recall)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1629, 23243, 399)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TP, FP, FN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0654953361209392, 0.8032544378698225, 0.12111524163568772)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision, recall, f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a model for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = config_parser('./config.ini') # parse config.ini\n",
    "parameters = process_parameters(config, {}) # process_parameters expects a dictionary\n",
    "\n",
    "sequence_processing_model = SequenceProcessingModel(**parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sequence_processing_model._load_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
