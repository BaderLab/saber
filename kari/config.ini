[mode]
# possible models: [LSTM-CRF, MT-LSTM-CRF, ]
model_name = LSTM-CRF
train_model = True
load_pretrained_model = False

[data]
dataset_text_folder = ../datasets/NCBI_disease, ../datasets/linnaeus
output_folder = ../output
# if load_pretrained_model, this will be ignored
pretrained_model_weights =

# in order to use random initialization instead, leave token_pretrained_embedding_filepath to blank, as below:
# token_pretrained_embedding_filepath =
token_pretrained_embedding_filepath = ../word_embeddings/wikipedia-pubmed-and-PMC-glove.txt
# note: if pre-trained embeddings are provided, the token embedding dimension will be the same size as these embeddings and this argument will be ignored.
token_embedding_dimension = 200

[training]
optimizer = sgd
activation_function = relu
learning_rate = 0.005
gradient_clipping_value = 0

dropout_rate = 0.3

batch_size = 1
k_folds = 5
maximum_number_of_epochs = 50

max_seq_len = 50

[advanced]
debug = False
freeze_token_embeddings = True
