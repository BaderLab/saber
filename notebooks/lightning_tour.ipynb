{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lightning Tour"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introduces the main ways of using Saber.\n",
    "\n",
    "### Table of contents\n",
    "\n",
    "1. [Training](#Training)\n",
    "    1. [Preparing the dataset](#Preparing-the-dataset)\n",
    "    2. [Preparing the embeddings](#Preparing-the-embeddings)\n",
    "    3. [Transfer learning](#Transfer-learning)\n",
    "    4. [Multi-task learning](#Multi-task-learning)\n",
    "2. [Saving and loading](#Saving-and-loading)\n",
    "    1. [Saving a model](#Saving-a-model)\n",
    "    2. [Loading a model](#Loading-a-model)\n",
    "3. [Performing predictions](#Preforming-predictions)\n",
    "4. [Visualizations](#Visualizations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Import the required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from saber.config import Config\n",
    "from saber.sequence_processor import SequenceProcessor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the dataset\n",
    "\n",
    "Saber requires all datasets to be provided in a CoNLL format with a BIO tag scheme, e.g.\n",
    "\n",
    "```\n",
    "Selegiline\tB-CHED\n",
    "-\tO\n",
    "induced\tO\n",
    "postural\tB-DISO\n",
    "hypotension\tI-DISO\n",
    "...\n",
    "```\n",
    "\n",
    "In this format, the first column contains each token of an input sentence, the last column contains the tokens tag, all columns are seperated by tabs, and all sentences by a newline. We have collected some corpora in this format [here](https://github.com/BaderLab/Biomedical-Corpora) for convenience.\n",
    "\n",
    "> Corpora in the **Standoff** format can be converted to **CoNLL** format using [this](https://github.com/spyysalo/standoff2conll) tool. Corpora in **PubTator** format can be converted to **Standoff** first using [this](https://github.com/spyysalo/pubtator) tool.\n",
    "\n",
    "Saber infers the \"training strategy\" based on the structure of the dataset folder:\n",
    "\n",
    "- To use k-fold cross-validation, simply provide a `train.*` file in your dataset folder.\n",
    "\n",
    "E.g.\n",
    "```\n",
    ".\n",
    "├── NCBI-Disease\n",
    "│   └── train.tsv\n",
    "```\n",
    "\n",
    "- To use a train/valid/test strategy, provide `train.*` and `test.*` files in your dataset folder. Optionally, you can provide a `valid.*` file. If not provided, a random 10% of examples from `train.*` are used as the validation set.\n",
    "\n",
    "E.g.\n",
    "```\n",
    ".\n",
    "├── NCBI-Disease\n",
    "│   ├── test.tsv\n",
    "│   └── train.tsv\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the embeddings\n",
    "\n",
    "When training new models, you can (and should) provide your own pre-trained word embeddings with the `pretrained_embeddings` argument (see below). Saber expects all word embeddings to be in the `word2vec` file format. [Pyysalo _et al_. 2013](https://pdfs.semanticscholar.org/e2f2/8568031e1902d4f8ee818261f0f2c20de6dd.pdf) provide word embeddings that work quite well in the biomedical domain, which can be downloaded [here](http://bio.nlplab.org). Alternatively, from the command line call:\n",
    "\n",
    "```bash\n",
    "$ mkdir saber/word_embeddings\n",
    "$ cd saber/word_embeddings\n",
    "# Note: this file is over 4GB\n",
    "$ wget http://evexdb.org/pmresources/vec-space-models/wikipedia-pubmed-and-PMC-w2v.bin\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: you do not need to download pre-trained word embeddings if you only plan on using Saber's pre-trained models.\n",
    "\n",
    "#### GloVe\n",
    "\n",
    "To use [GloVe](https://nlp.stanford.edu/projects/glove/) embeddings, just convert them to the [word2vec](https://code.google.com/archive/p/word2vec/) format first, e.g."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "glove_input_file = 'glove.txt'\n",
    "word2vec_output_file = 'word2vec.txt'\n",
    "glove2word2vec(glove_input_file, word2vec_output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a model from scratch\n",
    "\n",
    "Create a `SequenceProcessor` object. This object coordinates training, annotation, saving and loading of models and datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp = SequenceProcessor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If not specified, SequenceProcessor uses the default `config.ini`. Alternatively, you can write your own `config.ini` file and specify it as follows "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp = SequenceProcessor(Config('path/to/your/config.ini')) # you don't HAVE to name it 'config.ini'!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See [here](https://github.com/BaderLab/saber/blob/master/saber/config.ini) for the default `config.ini` file.\n",
    "\n",
    "> Note that if you are calling Saber from the command line, you can provide any parameter in the `config` file as a flag, e.g. `--config_filepath`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then load the dataset, which is specified by the `dataset_folder` parameter (see [Preparing the dataset](#Preparing-the-dataset)). If pre-trained token embeddings were specified with `pretrained_embeddings`, you should load them here also (see [Preparing the embeddings](#Preparing-the-embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp.load_dataset()\n",
    "# uncomment to load pre-trained word embeddings\n",
    "# sp.load_embeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note, you can actually pass the filepath to the dataset straight to the `load_dataset()` method, e.g. `sp.load_dataset('path/to/NCBI-Disease')`. The same is true for the `load_embeddings()` method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we create the model we would like to use (specified by the `model_name` parameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp.create_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Again, `model_name` can alternatively be passed directly to `create_model()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are then ready to train:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfer learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transfer learning is as easy as training, saving, loading, and then continuing training of a model. Here is an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train a model on GENIA corpus\n",
    "sp = SequenceProcessor()\n",
    "sp.load_dataset('path/to/datasets/GENIA')\n",
    "sp.create_model()\n",
    "sp.fit()\n",
    "sp.save('pretrained_models/GENIA')\n",
    "\n",
    "# Load that model\n",
    "del sp\n",
    "sp = SequenceProcessor()\n",
    "sp.load('pretrained_models/GENIA')\n",
    "\n",
    "# Use transfer learning to continue training on a new dataset\n",
    "sp.load_dataset('path/to/datasets/CRAFT')\n",
    "sp.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note that there is currently no way to easily do this with the command line interface, but I am working on it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-task learning\n",
    "\n",
    "Multi-task learning is as easy as specifying multiple dataset paths, either in the `config` file, at the command line via the flag `--dataset_folder`, or as an argument to `load_dataset()`. The number of datasets is arbitrary.\n",
    "\n",
    "Here is an example using the last method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp = SequenceProcessor()\n",
    "\n",
    "# Simply pass multiple dataset paths to load_dataset to use multi-task learning. \n",
    "sp.load_dataset('path/to/datasets/NCBI-Disease', 'path/to/datasets/Linnaeus')\n",
    "\n",
    "sp.create_model()\n",
    "sp.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and loading\n",
    "\n",
    "In the following sections we introduce the saving and loading of models.\n",
    "\n",
    "### Saving a model\n",
    "\n",
    "Assuming the model has already been created (see above), we can easily save our model like so"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_saved_model = 'path/to/pretrained_models/PRGE'\n",
    "\n",
    "sp.save(path_to_saved_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Currently, `sp.save()` will save the weights from the last training epoch by default."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading a model\n",
    "\n",
    "Lets illustrate loading a model with a new `SequenceProccesor` object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete our previous SequenceProccesor object (if it exists)\n",
    "if 'sp' in locals(): del sp\n",
    "\n",
    "# Create a new SequenceProccesor object\n",
    "sp = SequenceProcessor()\n",
    "\n",
    "# Load a previous model\n",
    "sp.load(path_to_saved_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preforming predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Library\n",
    "\n",
    "If you are using Saber as a `python` library, you can perform predictions on raw text with the `annotate()` method. Passing the argument `jupyter=True` allows us to render the result directly in the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming sp is a `SequenceProcessor` object with a loaded (and trained) model\n",
    "sp.annotate('Viral-mediated noisy gene expression reveals biphasic E2f1 response to MYC Gene expression.', jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Web-service\n",
    "\n",
    "Sabers web-service is invoked locally from the shell with\n",
    "\n",
    "```\n",
    "$ python -m saber.app\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> You have to run this from your terminal! Not in the notebook.\n",
    "\n",
    "See [here](https://baderlab.github.io/saber-api-docs/) for Sabers web-service API docs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizations\n",
    "\n",
    "_Note: This is less a feature and more a by-product of the fact that the model is implemented in [Keras](https://keras.io)._\n",
    "\n",
    "We can easily create an image depiction our model. First, install the [graphviz graph library](http://www.graphviz.org/) and the [Python interface](https://pypi.python.org/pypi/graphviz). This is useful if you plan on modifying the architecture of the model.\n",
    "\n",
    "> More info can be found [here](https://machinelearningmastery.com/visualize-deep-learning-neural-network-model-keras/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp = SequenceProcessor()\n",
    "\n",
    "# set this variable equal to your Keras model object.\n",
    "model_ = sp.model.model[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can either: create and save an image on our local machine,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "plot_model(model_, to_file='model.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or, visualize it directly in the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "SVG(model_to_dot(model_, show_shapes=True, show_layer_names=True).create(prog='dot', format='svg'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:saber]",
   "language": "python",
   "name": "conda-env-saber-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
