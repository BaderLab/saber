[mode]
# Possible models: [MT-LSTM-CRF, ]
model_name = MT-LSTM-CRF
train_model = True
load_pretrained_model = False

[data]
dataset_folder = ../datasets/MLEE_IOB ../datasets/BC2GM_IOB
output_folder = ../output
# If load_pretrained_model is false, this will be ignored
pretrained_model_weights =

# In order to use random initialization instead, leave token_pretrained_embedding_filepath to blank, as below:
token_pretrained_embedding_filepath =
# If pre-trained embeddings are provided, token_embedding_dimension will be the same size as these embeddings and this argument will be ignored.
token_embedding_dimension = 200
character_embedding_dimension = 25

[training]
optimizer = nadam
activation_function = relu
# Set to 0 to turn off gradient normalization
gradient_normalization = 1
# For certain optimizers, these values are ignored. See compile_model() in utils_models.py
learning_rate = 0.01
decay = 0.0

dropout_rate = 0.3

batch_size = 32
k_folds = 5
maximum_number_of_epochs = 60

[advanced]
verbose = False
debug = False
freeze_token_embeddings = True
max_char_seq_len = 15
