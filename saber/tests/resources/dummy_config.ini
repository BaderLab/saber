[mode]
# Possible models: [MT-LSTM-CRF, ]
model_name = MT-LSTM-CRF
# If True, model is compressed and saved in output_folder at the end of training. Model weights are
# taken from the last epoch.
save_model = False

[data]
# You can specify multiple datasets by listing their paths, separated by a comma.
dataset_folder = saber/tests/resources/dummy_dataset_1
output_folder = ../output
# Path to pre-trained model. To train a model from scratch, leave this blank.
pretrained_model =
# Path to pre-trained word embeddings. In order to use random initialization, leave this blank.
# Note that you can leave this blank when loading a pre-trained model
# (via 'SequenceProcessor.load()') that was trained with pre-tained embeddings
pretrained_embeddings = saber/tests/resources/dummy_word_embeddings/dummy_word_embeddings.txt

[model]
# If pre-trained word embeddings are provided, word_embed_dim will be the same size as these
# embeddings and this argument will be ignored.
word_embed_dim = 200
char_embed_dim = 30

[training]
# Values chosen for each hyperparameter represent sensible defaults that perform well across a wide
# range of NLP tasks (POS tagging, Chunking, NER, etc.) and thus should only be changed in special
# circumstances.
optimizer = nadam
activation = relu
# Set to 0 to turn off gradient normalization.
grad_norm = 1.0
# For certain optimizers, these values are ignored. See compile_model() in
# saber/utils/model_utils.py.
learning_rate = 0.0
decay = 0.0

# Three dropout values must be specified (separated by a comma), corresponding to the dropout rate
# to apply to the input, output and recurrent connections respectively. Must be a value between 0.0
# and 1.0.
dropout_rate = 0.3, 0.3, 0.1

batch_size = 32
# If a test partition is supplied at 'dataset_folder' (test.*) then this argument is ignored, and a
# simple train/valid/test scheme is used. A valid partition (valid.*) may optionally be provided
# along with the test parition. If none is found, 10% of examples are randomly selected.
k_folds = 2
epochs = 50

# Matching criteria used when determining whether or not a prediction is a true-positive. Choices
# are 'left' for left-boundary matching, 'right' for right-boundary matching and 'exact' for
# exact-boundary matching.
criteria = exact

[advanced]
verbose = False
debug = False
# If True, per-epoch logs which can be visualized with TensorBoard are written to output_folder
# Note: These logs can be quite large.
tensorboard = False
# If True, then during training the models weights for each and every epoch will be saved.
# Otherwise, weights are only saved for epochs that achieve a new best on validation loss.
save_all_weights = False
# If True, tokens that occur less than 1 time in the training dataset (hapax legomenon) are replaced
# with a special unknown token. This should result in faster loading times of pre-trained word
# embeddings and faster training times.
replace_rare_tokens = False
# If True, then all pre-trained word embeddings provided via pretrained_embeddings are loaded.
# Otherwise, only pre-trained embeddings for tokens found in the dataset(s) at dataset_folder are
# loaded. For evaluation, it's best to leave this as False. For models that will be deployed, it's
# best to set it to True.
load_all_embeddings = False
# If True, then pre-trained word embeddings will be fine-tuned with the other parameters of the
# neural network during training. Generally, you should not set this to True unless you have a very
# large training dataset.
# NOTE: if 'pretrained_embeddings' are not provided, they will be randomly initialized and
# fine-tuned during training, ignoring this argument.
fine_tune_word_embeddings = False
# TEMP. Set to true if variational dropout should be used.
variational_dropout = False
