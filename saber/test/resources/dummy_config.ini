[mode]
# Possible models: [MT-LSTM-CRF, ]
model_name = MT-LSTM-CRF
train_model = True
load_pretrained_model = False

[data]
dataset_folder = saber/test/resources/dummy_dataset
output_folder = ../output
# if load_pretrained_model is false, this will be ignored
pretrained_model_weights =

# In order to use random initialization instead, leave token_pretrained_embedding_filepath
# to blank, as below:
token_pretrained_embedding_filepath = saber/test/resources/dummy_word_embeddings/dummy_word_embeddings.txt

[model]
# If pre-trained embeddings are provided, token_embedding_dimension will be the
# same size as these embeddings and this argument will be ignored.
token_embedding_dimension = 200
character_embedding_dimension = 30

[training]
# Values chosen for each hyperparameter represent sensible defaults that perform
# well across a wide range of NLP tasks (POS tagging, Chunking, NER, etc.) and
# thus should only be changed in special circumstances.
optimizer = sgd
activation_function = relu
# Set to 0 to turn off gradient normalization
gradient_normalization = 0.0
# For certain optimizers, these values are ignored. See compile_model() in
# utils_models.py
learning_rate = 0.01
decay = 0.05

# Three dropout values must be specified, corresponding to the dropout rate to
# apply to the input, output and recurrent connections in that order. Must be a
# value between 0 and 1.
dropout_rate = 0.3, 0.3, 0.1

batch_size = 1
k_folds = 2
maximum_number_of_epochs = 10

[advanced]
verbose = False
debug = False
# If True, tokens that occur less than 1 time in the training dataset
# (hapax legomenon) are replaced with a special unknown token. This should
# result in faster loading times of pre-trained token embeddings and faster
# training times.
replace_rare_tokens = False
# If True, then pre-trained token embeddings will be fine-tuned with the other
# parameters of the neural network during training. Generally, you should not
# set this to True unless you have a very large training dataset.
trainable_token_embeddings = False
